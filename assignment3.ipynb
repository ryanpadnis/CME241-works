{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2024) - Assignment 3\n",
    "\n",
    "**Due: Jan 29 @ 11:59pm Pacific Time on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- **Please solve questions 1 and 2, and choose one of questions 3 or 4.**\n",
    "- Empty code blocks are for your use. Feel free to create more under each section as needed.\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/ryanpadnis/CME241-works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git push https://github.com/ryanpadnis/CME241-works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "**Analytic Optimal Actions and Cost.** \n",
    "Consider a continuous-states, continuous-actions, discrete-time, non-terminating MDP with state space as $\\mathbb{R}$ and action space as $\\mathbb{R}$. When in state $s\\in \\mathbb{R}$, upon taking action $a\\in \\mathbb{R}$, one transitions to next state $s' \\in \\mathbb{R}$ according to a normal distribution $s' \\sim \\mathcal{N}(s, \\sigma^2)$ for a fixed variance $\\sigma^2 \\in \\mathbb{R}^+$. The corresponding cost associated with this transition is $e^{as'}$, i.e., the cost depends on the action $a$ and the state $s'$ one transitions to. The problem is to minimize the infinite-horizon {\\em Expected Discounted-Sum of Costs} (with discount factor $\\gamma < 1$). For this assignment, solve this problem just for the special case of $\\gamma = 0$ (i.e., the myopic case) using elementary calculus. Derive an analytic expression for the optimal action in any state and the corresponding optimal cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "$\\bf{Solution:}$\n",
    "\n",
    "Before, we apply the Bellman Optimality Equation, we need to model this equation as an MDP.  For starters, our reward, $r\\in R$, is simply just the cost associated with the chosen transition.  We are given ($R_{t+1})| (S_{t + 1}, S_{t}, A_{t}) = e^{as'}$ , which is dependent on the state we transition to and the action we have taken.  We also know $\\mathbb{P}(S_{t+1} = s'|S_{t} = s, A_{t} = a) \\sim \\mathcal{N}(s, \\sigma^2)$.  The problem, as noted, is to minimize the infinite horizone expected discounted sum of costs.  But since we have $\\gamma = 0$, this implies the future expected rewards at any given state is solely depended on the immediate rewards in the next state.  We are lacking a deterministic policy as well, hence we can use the optimal value function equation: \n",
    "\n",
    "$V^*(s) = \\max\\limits_{a\\in \\mathbb{R}} \\left\\{ R(s,a) + \\gamma \\cdot \\sum_{s'} P(s,a,s') \\cdot V^*(s') \\right\\}$\n",
    "\n",
    "The beauty is, the gamma tends the far RHS to zero, hence we simply have:\n",
    "\n",
    "$V^*(s) = \\max\\limits_{a\\in \\mathbb{R}} \\left\\{ R(s,a)\\right\\}$\n",
    "\n",
    "Where minimizing the cost is the same as maximizing the negative cost (i.e. ($R_{t+1})| (S_{t + 1}, S_{t}, A_{t}) = -e^{as'}$ which is clearly negative for all actions and transition states).\n",
    "\n",
    "Now we must note, $R(s, a) = \\mathbb{E}[R_{t+1} | (S_t = s, A_t = a)]$.  This is still not a directly computable quantity with the information we have, so we will have to expand this out using the state transition probability function, which is continous and will thus require a density to compute, using the following integration formulation:\n",
    "\n",
    "$\\begin{align*}\n",
    "&= \\max\\limits_{a\\in \\mathbb{R}} \\int_{\\mathbb{R}} \\mathbb{P}(s, a, s')* R_{T}(s, a, s') \\, ds \\\\\n",
    "&= \\max\\limits_{a\\in \\mathbb{R}} \\int_{\\mathbb{R}} \\mathbb{P}(s, a, s')* \\sum_{r \\in D} \\frac{\\mathbb{P}(r, s, a, s')}{\\mathbb{P}(s, a, s')})* -e^{as} ds \\\\\n",
    "&= \\max\\limits_{a\\in \\mathbb{R}} \\int_{\\mathbb{R}} \\frac{\\mathbb{P}(s, a, s')}{{\\mathbb{P}(s, a, s')}}* \\sum_{r \\in D} *\\mathbb{P}(r, s, a, s'))* -e^{as}  ds \\\\\n",
    "&= \\max\\limits_{a\\in \\mathbb{R}} \\int_{\\mathbb{R}}  \\sum_{r \\in D} -e^{as}*\\mathbb{P}(r, s, a, s') ds \\\\\n",
    "&= \\max\\limits_{a\\in \\mathbb{R}} \\int_{\\mathbb{R}} -e^{as}*\\mathbb{P}(s, a, s') ds \\\\\n",
    "&\\rightarrow \\\\\n",
    "&= \\max\\limits_{a\\in \\mathbb{R}} \\int_{\\mathbb{R}} -e^{as}*\\mathbb{P}(S_{t+1} = s'|S_{t} = s, A_{t} = a) ds \\\\\n",
    "&= \\max\\limits_{a\\in \\mathbb{R}} \\int_{\\mathbb{R}} -e^{as}*\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(s'-s)^2}{2\\sigma^2}} ds\n",
    "\\end{align*}$\n",
    " \n",
    " We finally have obtained an integrable function over R, which we can now simply as follows making the substitution of $s --> s + \\frac{1}{2}a\\sigma$ s.t. $ds = ds$ which implies the differentials are still equal:\n",
    " \n",
    "$\\max_{a \\in \\mathbb{R}} \\left\\{ \\int_{-\\infty}^{+\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot e^{-\\frac{(x-(s+a\\sigma/2))^2}{2\\sigma^2}} \\cdot e^{sa + \\frac{\\sigma^2 a^2}{2}} \\, dx \\right\\} \\\\\n",
    "= \\max_{a \\in \\mathbb{R}} \\left\\{ -e^{sa + \\frac{\\sigma^2 a^2}{2}} \\cdot \\int_{-\\infty}^{+\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot e^{-\\frac{(x-(s+a\\sigma/2))^2}{2\\sigma^2}} \\, dx \\right\\} \\\\ $\n",
    "\n",
    "The far RHS is simply just the pdf evaluated everyone of a Normal Distribution PDF with mean $s + a\\sigma^{2}$ and variance $\\sigma^{2}$, hence it will just integrate to 1 by the definition of a PDF over the Real Line:\n",
    "\n",
    "$ = \\max_{a \\in \\mathbb{R}} \\left\\{ -e^{sa + \\frac{\\sigma^2 a^2}{2}} \\right\\}$\n",
    "\n",
    "Now to solve this, we employ univariate differentiation with respect to a:\n",
    "\n",
    "we find the value of $a$ that maximizes the expression:\n",
    "\n",
    "$\\\n",
    "g(a) = -e^{sa + \\frac{\\sigma^2 a^2}{2}}\n",
    "\\$\n",
    "\n",
    "we take the derivative of $g(a)$ with respect to $a$:\n",
    "\n",
    "$\\\n",
    "g'(a) = \\frac{d}{da} \\left( -e^{sa + \\frac{\\sigma^2 a^2}{2}} \\right)\n",
    "\\\n",
    "\\\n",
    "= \\left( s + \\sigma^2 a \\right) \\cdot (-e^{sa + \\frac{\\sigma^2 a^2}{2}})\n",
    "\\$\n",
    "\n",
    "Setting $g'(a)$ equal to zero, we have:\n",
    "\n",
    "$\\\n",
    "s + \\sigma^2 a = 0\n",
    "\\$\n",
    "\n",
    "Solving for $a^*$, the optimal action, we get:\n",
    "\n",
    "$\\\n",
    "\\boxed{a^* = -\\frac{s}{\\sigma^2}}\n",
    "\\$\n",
    "\n",
    "Therefore, the value of $a$ that maximizes the expression is $-\\frac{s}{\\sigma^2}$.  This implies the optimal action which minimizes the cost is $a^* = -\\frac{s}{\\sigma^2}$.  To find the optimal cost, we simply plug this in to see \n",
    "\n",
    "$V^*(s) = e^{argmax(V^*(s))s} = e^{(-\\frac{s}{\\sigma^2})s}$\n",
    "\n",
    "$\\boxed{V^*(s) = e^{-\\frac{s^{2}}{\\sigma^2}}}$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "**Manual Value Iteration.** \n",
    "Consider a simple MDP with $\\mathcal{S} = \\{s_1, s_2, s_3\\}, \\mathcal{T} =\\{s_3\\}, \\mathcal{A} = \\{a_1, a_2\\}$. The State Transition Probability function\n",
    "$$\\mathcal{P}: \\mathcal{N} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$$\n",
    "is defined as:\n",
    "$$\\mathcal{P}(s_1, a_1, s_1) = 0.2, \\mathcal{P}(s_1, a_1, s_2) = 0.6, \\mathcal{P}(s_1, a_1, s_3) = 0.2$$\n",
    "$$\\mathcal{P}(s_1, a_2, s_1) = 0.1, \\mathcal{P}(s_1, a_2, s_2) = 0.2, \\mathcal{P}(s_1, a_2, s_3) = 0.7$$\n",
    "$$\\mathcal{P}(s_2, a_1, s_1) = 0.3, \\mathcal{P}(s_2, a_1, s_2) = 0.3, \\mathcal{P}(s_2, a_1, s_3) = 0.4$$\n",
    "$$\\mathcal{P}(s_2, a_2, s_1) = 0.5, \\mathcal{P}(s_2, a_2, s_2) = 0.3, \\mathcal{P}(s_2, a_2, s_3) = 0.2$$\n",
    "The Reward Function \n",
    "$$\\mathcal{R}: \\mathcal{N} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$$\n",
    "is defined as:\n",
    "$$\\mathcal{R}(s_1, a_1) = 8.0, \\mathcal{R}(s_1, a_2) = 10.0$$\n",
    "$$\\mathcal{R}(s_2, a_1) = 1.0, \\mathcal{R}(s_2, a_2) = -1.0$$\n",
    "Assume discount factor $\\gamma = 1$.\n",
    "\n",
    "Your task is to determine an Optimal Deterministic Policy {\\em by manually working out} (not with code) simply the first two iterations of Value Iteration algorithm. \n",
    "\n",
    "- Initialize the Value Function for each state to be it's $\\max$ (over actions) reward, i.e., we initialize the Value Function to be $v_0(s_1) = 10.0, v_0(s_2) = 1.0, v_0(s_3) = 0.0$. Then manually calculate $q_k(\\cdot, \\cdot)$ and $v_k(\\cdot)$ from $v_{k - 1}( \\cdot)$ using the Value Iteration update, and then calculate the greedy policy $\\pi_k(\\cdot)$ from $q_k(\\cdot, \\cdot)$ for $k = 1$ and $k = 2$ (hence, 2 iterations).\n",
    "- Now argue that $\\pi_k(\\cdot)$ for $k > 2$ will be the same as $\\pi_2(\\cdot)$. Hint: You can make the argument by examining the structure of how you get $q_k(\\cdot, \\cdot)$ from $v_{k-1}(\\cdot)$. With this argument, there is no need to go beyond the two iterations you performed above, and so you can establish $\\pi_2(\\cdot)$ as an Optimal Deterministic Policy for this MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We can define the action value Function $Q$ as follows:\n",
    "$Q_k(s,a) =  { R(s,a) + \\gamma \\cdot \\sum_{s'} P(s,a,s') \\cdot V_{k-1}^(s') }$\n",
    "\n",
    "Then the find the corresponding state-value function, we can simply apply the maximum over all the action value functions:\n",
    "\n",
    "$V_{k}(s) = \\max\\limits_{a\\in \\mathbb{R}} Q_k(s,a)$\n",
    "\n",
    "Since we only have two actions and two nonterminal states, as well as a few number of reward transition probabilities, we can simplify this considerably:\n",
    "\n",
    "$Q_k(s_i,a_1) =  { R(s_i,a_1) + P(s_i,a_1,s_1) \\cdot V_{k-1}(s_1) + P(s_i,a_1,s_2) \\cdot V_{k-1}(s_2) }$\n",
    "\n",
    "$Q_k(s_i,a_2) =  { R(s_i,a_2) + P(s_i,a_2,s_1) \\cdot V_{k-1}(s_1) + P(s_i,a_2,s_2) \\cdot V_{k-1}(s_2) }$\n",
    "\n",
    "$V_{k}(s_i) = \\max \\left\\{ Q_k(s_i,a_1), Q_k(s_i, a_2)\\right\\}$\n",
    "\n",
    "Where $s_i$ is any of the states in the state space from 1 to 3. After we find this, which ever determinstic policy, $a_1$ or $a_2$, that contributes to the maximum value function will be our $\\pi_k(s)$.  Using the given\n",
    "\n",
    "$ Q_1(s_1, a_1) = 8.0 + 0.2\\times 10.0 + 0.6 \\times 1.0 = 10.6$ \n",
    "\n",
    "$Q_1(s_1, a_2) = 10.0 + 0.1 \\times 10.0 + 0.2 \\times 1.0 = 11.2 $\n",
    "\n",
    "$V_1(s_1) = \\max \\left\\{10.6, 11.2\\right\\} = 11.2$ \n",
    "\n",
    "$\\pi_1(s_1) = a_2$\n",
    "\n",
    "$Q_1(s_2, a_1) = 1.0 + 0.3 \\times 10.0 + 0.3 \\times 1.0 = 4.3$\n",
    "\n",
    "$Q_1(s_2, a_2) = -1.0 + 0.5 \\times 10.0 + 0.3 \\times 1.0 = 4.3$\n",
    "\n",
    "$V_1(s_2) = \\max \\left\\{4.3, 4.3\\right\\} = 4.3$ \n",
    "\n",
    "$\\pi_1(s_2) = a_1$\n",
    "\n",
    "This is our first value iteration.  Now we use the value function of $V_1(s_1) = 11.2 , V_1(s_2) = 4.3 , V_1(s_3) = 0$ to conduct our second iteration:\n",
    "\n",
    "$q_2(s_1, a_1) = 8.0 + 0.2 \\times 11.2 + 0.6 \\times 4.3 = 12.82 $\n",
    "\n",
    "$q_2(s_1, a_2) = 10.0 + 0.1 \\times 11.2 + 0.2 \\times 4.3 = 11.98 $\n",
    "\n",
    "$v_2(s_1) = \\max \\left\\{12.82, 11.98\\right\\} = 12.82 $\n",
    "\n",
    "$\\pi_2(s_1) = a_1 $\n",
    "\n",
    "$q_2(s_2, a_1) = 1.0 + 0.3 \\times 11.2 + 0.3 \\times 4.3 = 5.65 $\n",
    "\n",
    "$q_2(s_2, a_2) = -1.0 + 0.5 \\times 11.2 + 0.3 \\times 4.3 = 5.89 $\n",
    "\n",
    "$v_2(s_2) = \\max \\left\\{5.65, 5.89\\right\\} = 5.89 $\n",
    "\n",
    "$\\pi_2(s_2) = a_2 $\n",
    "\n",
    "And that concludes our value iteration algorithm for $k = 2$.  To argue that the policy $\\pi_k(a)$ for $k > 2$ will be the same as $\\pi_2(a)$, we need to examine how $q_k(\\cdot,\\cdot)$ is derived from $v_{k-1}(\\cdot)$, as per the hint.\n",
    "\n",
    "In each iteration, the Q-values $q_k(\\cdot,\\cdot)$ are calculated based on the previous iteration's value function $v_{k-1}(\\cdot)$ and the reward and transition probabilities of the MDP. The policy $\\pi_k(\\cdot)$ is then determined by selecting the action that maximizes the Q-value for each state.\n",
    "\n",
    "Now, since the MDP is stationary and the environment does not change over time, the transition probabilities and rewards remain constant. Therefore, as we iterate and update the Q-values and value functions, the structure of the MDP remains the same.\n",
    "\n",
    "This means that once an optimal policy $\\pi_2(\\cdot)$ is established, the Q-values and value functions calculated in subsequent iterations will not change the optimal actions for each state, as the underlying MDP structure remains unchanged. Thus, the policy $\\pi_k(\\cdot)$ for $k > 2$ will be the same as $\\pi_2(\\cdot)$.\n",
    "\n",
    "In other words, since the environment and rewards do not change, the optimal policy identified in iteration 2 will continue to be optimal in all subsequent iterations. Therefore, we can establish $\\pi_2(\\cdot)$ as an Optimal Deterministic Policy for this MDP.  Essentially, by our environment's stattionarity property, once our algorithm changes the greedy policy between iterations, we will never go back.  There are only two possible actions, hence we have established the result.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "**Job-Hopping and Wages-Utility-Maximization.** \n",
    "You are a worker who starts every day either employed or unemployed. If you start your day employed, you work on your job for the day (one of $n$ jobs, as elaborated later) and you get to earn the wage of the job for the day. However, at the end of the day, you could lose your job with probability $\\alpha \\in [0,1]$, in which case you start the next day unemployed. If at the end of the day, you do not lose your job (with probability $1-\\alpha$), then you will start the next day with the same job (and hence, the same daily wage). On the other hand, if you start your day unemployed, then you will be randomly offered one of $n$ jobs with daily wages $w_1, w_2, \\ldots w_n \\in \\mathbb{R}^+$ with respective job-offer probabilities $p_1, p_2, \\ldots p_n \\in [0,1]$ (with $\\sum_{i=1}^n p_i = 1$). You can choose to either accept or decline the offered job. If you accept the job-offer, your day progresses exactly like the {\\em employed-day} described above (earning the day's job wage and possibly (with probability $\\alpha$) losing the job at the end of the day). However, if you decline the job-offer, you spend the day unemployed, receive the unemployment wage $w_0 \\in \\mathbb{R}^+$ for the day, and start the next day unemployed. The problem is to identify the optimal choice of accepting or rejecting any of the job-offers the worker receives, in a manner that maximizes the infinite-horizon {\\em Expected Discounted-Sum of Wages Utility}. Assume the daily discount factor for wages (employed or unemployed) is $\\gamma \\in [0,1)$. Assume Wages Utility function to be $U(w) = \\log(w)$ for any wage amount $w \\in \\mathbb{R}^+$. So you are looking to maximize\n",
    "$$\\mathbb{E}[\\sum_{u=t}^\\infty \\gamma^{u-t} \\cdot \\log(w_{i_u})]$$\n",
    "at the start of a given day $t$ ($w_{i_u}$ is the wage earned on day $u$, $0\\leq i_u \\leq n$ for all $u\\geq t$).\n",
    "\n",
    "- Express with clear mathematical notation the state space, action space, transition function, reward function, and write the Bellman Optimality Equation customized for this MDP.\n",
    "- You can solve this Bellman Optimality Equation (hence, solve for the Optimal Value Function and the Optimal Policy) with a numerical iterative algorithm (essentially a Dynamic Programming algorithm customized to this problem). Write Python code for this numerical algorithm. Clearly define the inputs and outputs of your algorithm with their types (int, float, List, Mapping etc.). For this problem, don't use any of the MDP/DP code from the git repo, write this customized algorithm from scratch.\n",
    "\n",
    "\n",
    "\n",
    "The state space consists of two possible states: employed and unemployed. Let $S=\\{E,U\\}$, where $E$ represents being employed and $U$ represents being unemployed.\n",
    "\n",
    "The action space depends on the current state:\n",
    "If the worker is employed, there is only one action: continue working in the current job. Let $A_E = \\{\\text{Continue}\\}$.\n",
    "    \\item If the worker is unemployed, the action space consists of accepting or rejecting job offers. Let $A_U = \\{\\text{Accept}, \\text{Reject}\\}$ denote the action spaces for being unemployed.\n",
    "\n",
    "The transition function specifies the probability of moving from one state to another based on the action taken:\n",
    "\n",
    "If the worker continues in the current job, the transition probability from employed to employed is $(1 - \\alpha)$, and from employed to unemployed is $\\alpha$.  If the worker accepts a job offer, the transition probability from unemployed to employed depends on the job-offer probabilities.\n",
    "\n",
    "If the worker rejects a job offer, the transition probability from unemployed to unemployed is 1. Let $T(s,a,s')$ denote the transition probability from state $s$ to state $s'$ given action $a$.\n",
    "\n",
    "\n",
    "he reward function specifies the immediate reward received by the worker based on the action taken and the resulting state:\n",
    "\n",
    "If the worker continues in the current job, the reward is the daily wage of the job.\n",
    "If the worker accepts a job offer, the reward is the daily wage of the accepted job.\n",
    "If the worker rejects a job offer, the reward is the unemployment wage. Let $R(s,a)$ denote the immediate reward received when taking action $a$ in state $s$.\n",
    "\n",
    "The optimal value function, denoted by $V^*(s)$, represents the expected discounted sum of wages utility from a given state following the optimal policy.\n",
    "\n",
    "The optimal policy, denoted by $\\pi^*(s)$, represents the action that maximizes the expected discounted sum of wages utility from a given state.\n",
    "\n",
    "Now, we can write the Bellman Optimality Equation customized for this MDP:\n",
    "$V^*(s) = \\max \\left\\{ \\sum_{s'} T(s,a,s') \\left[ R(s,a) + \\gamma V^*(s') \\right] \\right\\}$\n",
    "\n",
    "where $s \\in S$ is the current state, $a \\in A$ is the action taken, $T(s,a,s')$ is the transition probability from state $s$ to state $s'$ given action $a$, $R(s,a)$ is the immediate reward received when taking action $a$ in state $s$, $\\gamma$ is the daily discount factor, and $V^*(s)$ is the optimal value function.  Below is my implementation of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('E', 0): 47.36842105263023, ('E', 1): 56.842105263156284, ('E', 2): 71.05263157894535, ('U',): 49.999991549908714}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def value_iteration(n: int, alpha: float, wages: List[float], job_probs: List[float], unemployment_wage: float, gamma: float, tolerance: float = 1e-6) -> Dict[str, float]:\n",
    "    # Initialize the value function\n",
    "    V = {('E', i): 0.0 for i in range(n)}  # Employed states\n",
    "    V.update({('U',): 0.0})  # Unemployed state\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "\n",
    "        # Iterate over all states\n",
    "        for s in V.keys():\n",
    "            if s[0] == 'E':\n",
    "                # Employed state\n",
    "                max_value = float('-inf')\n",
    "\n",
    "                # Calculate expected value for continuing in current job\n",
    "                reward_continue = wages[s[1]]\n",
    "                expected_value_continue = (1 - alpha) * (reward_continue + gamma * V[s])\n",
    "\n",
    "                max_value = max(max_value, expected_value_continue)\n",
    "\n",
    "            else:\n",
    "                # Unemployed state\n",
    "                max_value = float('-inf')\n",
    "\n",
    "                # Calculate expected value for accepting each job offer\n",
    "                for i in range(n):\n",
    "                    reward_accept = wages[i]\n",
    "                    expected_value_accept = job_probs[i] * (reward_accept + gamma * V[('E', i)])\n",
    "\n",
    "                    max_value = max(max_value, expected_value_accept)\n",
    "\n",
    "                # Calculate expected value for rejecting job offer\n",
    "                expected_value_reject = unemployment_wage + gamma * V[('U',)]\n",
    "\n",
    "                max_value = max(max_value, expected_value_reject)\n",
    "\n",
    "            V_new[s] = max_value\n",
    "            delta = max(delta, abs(max_value - V[s]))\n",
    "\n",
    "        V = V_new\n",
    "\n",
    "        if delta < tolerance:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "# Examples\n",
    "\n",
    "n = 3  # Number of jobs\n",
    "alpha = 0.1  # Probability of losing job\n",
    "wages = [10, 12, 15]  # Daily wages for each job\n",
    "job_probs = [0.4, 0.3, 0.3]  # Job offer probabilities\n",
    "unemployment_wage = 5  # Unemployment wage\n",
    "gamma = 0.9  # Daily discount factor\n",
    "\n",
    "optimal_value_function = value_iteration(n, alpha, wages, job_probs, unemployment_wage, gamma)\n",
    "print(optimal_value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "**Two-Stores Inventory Control.** \n",
    "We extend the capacity-constrained inventory example implemented in [rl/chapter3/simple_inventory_mdp_cap.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter3/simple_inventory_mdp_cap.py) as a `FiniteMarkovDecisionProcess` (the Finite MDP model for the capacity-constrained inventory example is described in detail in Chapters 1 and 2 of the RLForFinanceBook). Here we assume that we have two different stores, each with their own separate capacities $C_1$ and $C_2$, their own separate Poisson probability distributions of demand (with means $\\lambda_1$ and $\\lambda_2$), their own separate holding costs $h_1$ and $h_2$, and their own separate stockout costs $p_1$ and $p_2$. At 6pm upon stores closing each evening, each store can choose to order inventory from a common supplier (as usual, ordered inventory will arrive at the store 36 hours later). We are also allowed to transfer inventory from one store to another, and any such transfer happens overnight, i.e., will arrive by 6am next morning (since the stores are fairly close to each other). Note that the orders are constrained such that following the orders on each evening, each store's inventory position (sum of on-hand inventory and on-order inventory) cannot exceed the store's capacity (this means the action space is constrained to be finite). Each order made to the supplier incurs a fixed transportation cost of $K_1$ (fixed-cost means the cost is the same no matter how many units of non-zero inventory a particular store orders). Moving any non-zero inventory between the two stores incurs a fixed transportation cost of $K_2$. \n",
    "\n",
    "Model this as a derived class of `FiniteMarkovDecisionProcess` much like we did for `SimpleInventoryMDPCap` in the code repo. Set up instances of this derived class for different choices of the problem parameters (capacities, costs etc.), and determine the Optimal Value Function and Optimal Policy by invoking the function `value_iteration` (or `policy_iteration`) from file [rl/dynamic_programming.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/dynamic_programming.py).\n",
    "\n",
    "Analyze the obtained Optimal Policy and verify that it makes intuitive sense as a function of the problem parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
